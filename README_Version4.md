# ğŸ­ MMER â€” Multimodal Emotion Recognition

Welcome to **MMER (Multimodal Emotion Recognition)** â€” a unified AI system that detects emotions from **text, audio, image, and video** 

---

## ğŸ“ Folder Structure

The repository is organized as follows:

```
MMER/
â”œâ”€â”€ app.py
â”œâ”€â”€ audio_emotion.py
â”œâ”€â”€ image_emotion.py
â”œâ”€â”€ text_emotion.py
â”œâ”€â”€ video_emotion.py
â”œâ”€â”€ text_train.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ text_model.h5
â”‚   â”œâ”€â”€ audio_model.h5
â”‚   â”œâ”€â”€ image_model.h5
â”‚   â”œâ”€â”€ video_model.h5
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”œâ”€â”€ js/
â”‚   â””â”€â”€ images/
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ text.html
â”‚   â”œâ”€â”€ audio.html
â”‚   â”œâ”€â”€ image.html
â”‚   â””â”€â”€ video.html
```

### ğŸ§© File Descriptions

* **`app.py`** â€“ Main Flask application that connects all modalities.
* **`text_emotion.py`** â€“ Handles text emotion prediction using `text_model.h5`.
* **`audio_emotion.py`** â€“ Extracts MFCCs and predicts emotion using `audio_model.h5`.
* **`image_emotion.py`** â€“ Performs facial emotion recognition using `image_model.h5`.
* **`video_emotion.py`** â€“ Analyzes sampled video frames and predicts overall emotion using `video_model.h5`.
* **`text_train.py`** â€“ Script for training and saving the text-based emotion model.
* **`requirements.txt`** â€“ Contains all dependencies.
* **`models/`** â€“ Folder containing all pre-trained `.h5` models.
* **`static/` & `templates/`** â€“ Frontend assets (CSS, JS, HTML templates).

---

## ğŸš€ Features

âœ… **Multimodal Emotion Recognition**

* Text, audio, image, and video support
* Deep learning models (`.h5` loaded via TensorFlow/Keras)

âœ… **Modular Architecture**

* Each modality handled by a dedicated Python module
* Easy to update or replace individual models

âœ… **Flask-Based Web Interface**

* Upload files or enter text directly in browser
* Real-time emotion predictions with confidence levels

âœ… **Scalable Design**

* Add new modalities easily (e.g., physiological sensors, EEG)
* Ready for future API or mobile app integration

---

## âš™ï¸ Getting Started

Follow these steps to run the MMER app locally:

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/kalyan09122003/MMER.git
cd MMER
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Add Your Trained Models

Place your `.h5` model files inside the `models/` folder:

```
models/
â”œâ”€â”€ text_model.h5
â”œâ”€â”€ audio_model.h5
â”œâ”€â”€ image_model.h5
â””â”€â”€ video_model.h5
```

### 4ï¸âƒ£ Run the Flask Application

```bash
python app.py
```

Then open your browser and go to:

```
http://localhost:5000
```

---

## ğŸ§  Model Details

| Modality  | Model File       | Input Type      | Preprocessing                   |
| --------- | ---------------- | --------------- | ------------------------------- |
| **Text**  | `text_model.h5`  | Raw text        | Tokenization + Padding          |
| **Audio** | `audio_model.h5` | `.wav`, `.webm` | MFCC Extraction                 |
| **Image** | `image_model.h5` | `.jpg`, `.png`  | Face detection + Resize (48x48) |
| **Video** | `video_model.h5` | `.mp4`, `.avi`  | Frame sampling + Resize (48x48) |

---

## ğŸ§© API Endpoints

| Endpoint         | Method | Input                                    | Output                         |
| ---------------- | ------ | ---------------------------------------- | ------------------------------ |
| `/predict_text`  | POST   | JSON (`{"text": "I'm so happy today!"}`) | Emotion label + probabilities  |
| `/predict_audio` | POST   | Audio file                               | Emotion label + probabilities  |
| `/predict_image` | POST   | Image file                               | Emotion label + probabilities  |
| `/video`         | POST   | Video file                               | Dominant emotion across frames |

---



## ğŸ¤ Contributing

We welcome contributions!
To contribute:

1. **Fork** this repository
2. **Create** your feature branch

   ```bash
   git checkout -b feature/YourFeature
   ```
3. **Commit** your changes

   ```bash
   git commit -am "Add YourFeature"
   ```
4. **Push** to your branch

   ```bash
   git push origin feature/YourFeature
   ```
5. **Open a Pull Request**

---

## ğŸ“œ License

This project is released under the **MIT License**.
Feel free to use, modify, and distribute for educational and research purposes.

---



---
